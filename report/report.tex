\documentclass[a4paper]{article}
\usepackage{amsmath, amssymb, bm}
\usepackage[margin=1in]{geometry}
\usepackage[table,xcdraw]{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\begin{document}
\begin{titlepage}
  \centering
    {\huge \bf Assignment 3\par}
    \vspace{1cm}
    {\Large Computational Intelligence, SS2018\par}
    \vspace{1cm}
    \begin{tabular}{|l|l|l|}
      \hline
      \multicolumn{3}{|c|}{\textbf{Team Members}}   \\ \hline
      Last name & First name & Matriculation Number \\ \hline
      Lee       & Eunseo     & 11739623             \\ \hline
      Shadley   & Alex       & 11739595             \\ \hline
      Lee       & Dayeong    & 11730321             \\ \hline
    \end{tabular}
\end{titlepage}

\section{Linear SVM}
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{1_datapoint.png}
    \caption{Data points}
    \includegraphics[width=0.5\textwidth]{1_a.png}
    \caption{Plot for a}
    \includegraphics[width=0.5\textwidth]{1_b.png}
    \caption{Plot for b}
  \end{center}
\end{figure}

\noindent
The decision boundary changed almost horizontal to X1 when the new point because
the original decision boundary cannot classify the new point correctly.\\
When $C$ is large, it means minimizing of $\sum_{i=1}^{m}\xi_i$ is much important
than minimizing of $||w||^2$. It means the decision boundary tries to minimize
the size and the number of the margin constraint violations when $C$ is large.
Thus, the margin will be narrower, the number of support vectors and samples
that violates margin constraint will be smaller.

\clearpage

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{1_c_6.png}
    \includegraphics[width=0.5\textwidth]{1_c_0.png}
    \includegraphics[width=0.5\textwidth]{1_c_-1.png}
    \includegraphics[width=0.5\textwidth]{1_c_-3.png}
    \caption{Plot when $C=10^6, 10^0, 10^{-1}, 10^{-3}$}
  \end{center}
\end{figure}

\clearpage

\section{Nonlinear (kernal) SVM}

\subsection*{Results}

\noindent
\includegraphics[width=0.5\textwidth]{nonlinear_data.png}%
\includegraphics[width=0.5\textwidth]{linear_fit.png}\\[2em]
\includegraphics[width=0.5\textwidth]{rbf_error.png}%
\includegraphics[width=0.5\textwidth]{rbf_fit.png}\\[2em]
\includegraphics[width=0.5\textwidth]{poly_error.png}%
\includegraphics[width=0.5\textwidth]{poly_fit.png}\par

The above figures depict (left to right, top to bottom)

\begin{enumerate}
	\item Training data used for all algorithms
	\item A Linear SVM convergence
	\item Error in a RBF SVM plotted against the gamma value used in the kernel
	\item The most proficient RBF SVM
	\item Error in a Polynomial SVM plotted against the polynomial degree used in the kernel
	\item The most proficient Polynomial SVM
\end{enumerate}


\subsection*{Discussion}

Our Polynomial SVM had the best performance with a score of .9575 with degree 9.  The RBF SVM performed second best, scoring .9425 with a gamma of 1.85, and the Linear SVM exhibited the worst performance at .8125 .  Since the data is not linearly seperable, it is clear why the Linear SVM performed poorly.  The polynomial model outperformed the RBF model because it was able to more accurately model the central region $(-.5 < x_1, x_2 < .5)$.  It is worth noting that both models struggled with this region, with both having a significant number of misclassified examples in the region.

In general, it can be seen that a high number of Support Vectors correlates to complex decision boundaries, although this is obviously not the case for linear models.  Despite the testing results, I believe the RBF model produces a better generalization of the data, as it places the decision boundary more accurately between data points than other models.

\section{Multiclass classification}
\end{document}
